{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModel, pipeline, BertForSequenceClassification\n",
    "from transformers import logging as hflogging\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Data/pairs_with_ratings.tsv\",sep=\"\\t\")  # CSV with columns 'text' and 'label'\n",
    "data = data.drop_duplicates([\"original_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ([1]*len(data[\"original_title\"]))+([0]*len(data[\"title\"]))\n",
    "tiltes = (data[\"original_title\"] + data[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Partially Faded Hand Stamp Undermining Everyth...\n",
       "7       Synthesizer Trumpet Announces Arrival Of Porn ...\n",
       "15      Depressed Security Guard Turns Big Flashlight ...\n",
       "22      Fugitive Doctor Accuses Devlin MacGregor Of Fr...\n",
       "30      AIG Nearly Blows All The Goodwill Built Up By ...\n",
       "                              ...                        \n",
       "5653    Red Lobster Celebrates Return Of Annual All-Yo...\n",
       "5658    Breaking: LeBron James Leaning Toward Joining ...\n",
       "5660    Study Finds People On Dates Know Within 30 Sec...\n",
       "5664    Man Needs Verbal Assurance That Hand Stamp Wil...\n",
       "5666    God Pledges $5,000 For Cancer ResearchGATES PL...\n",
       "Length: 1191, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([data[\"original_title\"], data[\"title\"]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1191\n",
      "2382\n"
     ]
    }
   ],
   "source": [
    "print(len(tiltes))\n",
    "print(len(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = pd.DataFrame({\"titles\":pd.concat([data[\"original_title\"], data[\"title\"]], axis=0),\n",
    "                         \"humor\":[1]*len(data[\"original_title\"])+[0]*len(data[\"title\"])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "# Load your dataset from a CSV file\n",
    "data = pd.read_csv(\"Data/pairs_with_ratings.tsv\",sep=\"\\t\")  # CSV with columns 'text' and 'label'\n",
    "\n",
    "data = data.drop_duplicates([\"original_id\"])\n",
    "\n",
    "clean_data = pd.DataFrame({\"titles\":pd.concat([data[\"original_title\"], data[\"title\"]], axis=0),\n",
    "                         \"label\":[1]*len(data[\"original_title\"])+[0]*len(data[\"title\"])})\n",
    "\n",
    "# Split dataset into train and test\n",
    "train_df, test_df = train_test_split(clean_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert pandas DataFrame to HuggingFace Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'titles': Value(dtype='string', id=None),\n",
       " 'humor': Value(dtype='int64', id=None),\n",
       " '__index_level_0__': Value(dtype='int64', id=None),\n",
       " 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
       " 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\n",
       " 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1905/1905 [00:00<00:00, 14583.09 examples/s]\n",
      "Map: 100%|██████████| 477/477 [00:00<00:00, 12805.93 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the BERT tokenizer\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the data\n",
    "def preprocess_data(examples):\n",
    "    return tokenizer(examples['titles'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Apply preprocessing\n",
    "train_dataset = train_dataset.map(preprocess_data, batched=True)\n",
    "test_dataset = test_dataset.map(preprocess_data, batched=True)\n",
    "\n",
    "# Set the format for PyTorch tensors\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_encoded = model.lmtokenizer(train_dataset[\"text\"],\n",
    "                            truncation=True,\n",
    "                            padding=False,\n",
    "                            add_special_tokens=False,\n",
    "                            return_tensors=None,\n",
    "                            return_offsets_mapping=False,\n",
    "                        )\n",
    "\n",
    "X_test_encoded = model.lmtokenizer(test_dataset[\"text\"],\n",
    "                            truncation=True,\n",
    "                            padding=False,\n",
    "                            add_special_tokens=False,\n",
    "                            return_tensors=None,\n",
    "                            return_offsets_mapping=False,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load BERT pre-trained model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Define the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "trainer.evaluate()\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"./humor-classifier\")\n",
    "tokenizer.save_pretrained(\"./humor-classifier\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Explanation of the Script\n",
    "Loading the Dataset:\n",
    "\n",
    "We load the humor dataset from a CSV file with two columns: text and label. Then, we split the dataset into training and testing sets using train_test_split.\n",
    "Tokenization:\n",
    "\n",
    "We use BERT's tokenizer (BertTokenizer.from_pretrained) to preprocess the text data. The preprocess_data function tokenizes each text entry, truncates it to a maximum length of 128 tokens, and adds padding where necessary.\n",
    "Model Initialization:\n",
    "\n",
    "We load the pre-trained BERT model for sequence classification using BertForSequenceClassification. The model is initialized with two output labels (num_labels=2) because we have a binary classification task (humorous or non-humorous).\n",
    "Training Arguments:\n",
    "\n",
    "We define the TrainingArguments with important parameters like learning rate (2e-5), batch size (16), number of epochs (3), and evaluation strategy.\n",
    "Training the Model:\n",
    "\n",
    "The Trainer class from Hugging Face simplifies the training process. It takes care of the training loop, evaluation, and logging. We pass our model, training arguments, and datasets to the Trainer.\n",
    "Evaluation:\n",
    "\n",
    "The trainer.evaluate() function evaluates the model on the test dataset.\n",
    "Saving the Model:\n",
    "\n",
    "After training, the model and tokenizer are saved so they can be reused later for prediction or further fine-tuning.\n",
    "Step 4: Make Predictions on New Data\n",
    "After fine-tuning the model, you can use it to predict whether new text samples are humorous or non-humorous:\n",
    "\n",
    "python\n",
    "Copy code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BertForSequenceClassification' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load the fine-tuned model and tokenizer\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mBertForSequenceClassification\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./humor-classifier\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./humor-classifier\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Example text\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BertForSequenceClassification' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained(\"./humor-classifier\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./humor-classifier\")\n",
    "\n",
    "# Example text\n",
    "texts = [\"Why don’t scientists trust atoms? Because they make up everything!\", \n",
    "         \"The weather is nice today.\"]\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=128)\n",
    "\n",
    "# Predict\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "# Convert predictions to labels\n",
    "labels = ['humorous' if pred == 1 else 'non-humorous' for pred in predictions]\n",
    "\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBinaryClassifier(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, plm_name: str):\n",
    "        super(TransformerBinaryClassifier, self).__init__()\n",
    "        self.lmconfig = AutoConfig.from_pretrained(plm_name)\n",
    "        self.lmtokenizer = AutoTokenizer.from_pretrained(plm_name)\n",
    "        self.lm = AutoModel.from_pretrained(plm_name, output_attentions=False)\n",
    "        self.emb_dim = self.lmconfig.hidden_size\n",
    "        self.output_size = 1\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(0.2),\n",
    "            torch.nn.Linear(self.emb_dim, self.output_size),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "        self.loss_fn = torch.nn.BCELoss(reduction='mean')\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        #  x['input_ids']  -> N,T,d\n",
    "        x : torch.Tensor = self.lm(x['input_ids'], x['attention_mask']).last_hidden_state\n",
    "        global_vects = x.mean(dim=1)   #  N,d\n",
    "        x = self.classifier(global_vects)\n",
    "        return x.squeeze(-1)\n",
    "\n",
    "    def compute_loss(self, predictions, target):\n",
    "        return self.loss_fn(predictions, target)\n",
    "    \n",
    "model = TransformerBinaryClassifier(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andre\\Documents\\Cours\\SSD\\M2\\ProjetM2\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\andre\\.cache\\huggingface\\hub\\datasets--imdb. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Generating train split: 100%|██████████| 25000/25000 [00:00<00:00, 306491.84 examples/s]\n",
      "Generating test split: 100%|██████████| 25000/25000 [00:00<00:00, 371277.23 examples/s]\n",
      "Generating unsupervised split: 100%|██████████| 50000/50000 [00:00<00:00, 358730.35 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "ds_train = datasets.load_dataset(\"imdb\", split='train')\n",
    "ds_test = datasets.load_dataset(\"imdb\", split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['neg', 'pos'], id=None)}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_test.features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
