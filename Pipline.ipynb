{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModel, pipeline, BertForSequenceClassification\n",
    "from transformers import logging as hflogging\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from transformers import DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a custome classifier object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBinaryClassifier(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, plm_name: str):\n",
    "        super(TransformerBinaryClassifier, self).__init__()\n",
    "        self.lmconfig = AutoConfig.from_pretrained(plm_name)\n",
    "        self.lmtokenizer = AutoTokenizer.from_pretrained(plm_name)\n",
    "        self.lm = AutoModel.from_pretrained(plm_name, output_attentions=False)\n",
    "        self.emb_dim = self.lmconfig.hidden_size\n",
    "        self.output_size = 1\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(0.2),\n",
    "            torch.nn.Linear(self.emb_dim, self.output_size),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "        self.loss_fn = torch.nn.BCELoss(reduction='mean')\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        #  x['input_ids']  -> N,T,d\n",
    "        x : torch.Tensor = self.lm(x['input_ids'], x['attention_mask']).last_hidden_state\n",
    "        global_vects = x.mean(dim=1)   #  N,d\n",
    "        x = self.classifier(global_vects)\n",
    "        return x.squeeze(-1)\n",
    "\n",
    "    def compute_loss(self, predictions, target):\n",
    "        return self.loss_fn(predictions, target)\n",
    "    \n",
    "model = TransformerBinaryClassifier(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data, formating it into a dataset type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Data/pairs_with_ratings.tsv\",sep=\"\\t\")\n",
    "data = data.drop_duplicates([\"original_id\"])\n",
    "data = pd.DataFrame({\"text\":pd.concat([data[\"original_title\"], data[\"title\"]], axis=0),\n",
    "                         \"label\":[1]*len(data[\"original_title\"])+[0]*len(data[\"title\"])})\n",
    "# Split dataset into train and test\n",
    "train_df, test_df = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert pandas DataFrame to HuggingFace Dataset\n",
    "ds_train = Dataset.from_pandas(train_df)\n",
    "ds_test = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform text into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1905/1905 [00:00<00:00, 47913.78 examples/s]\n",
      "Map: 100%|██████████| 477/477 [00:00<00:00, 44192.50 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return model.lmtokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "\n",
    "# ds_train = ds_train.rename_column(\"label\", \"labels\")\n",
    "# ds_test = ds_test.rename_column(\"label\", \"labels\")\n",
    "\n",
    "# tokenize datasets\n",
    "tok_ds_train = ds_train.map(tokenize_function, batched=True)\n",
    "tok_ds_test = ds_test.map(tokenize_function, batched=True)\n",
    "\n",
    "tok_ds_train = tok_ds_train.remove_columns([\"text\"])\n",
    "tok_ds_test = tok_ds_test.remove_columns([\"text\"])\n",
    "\n",
    "tok_ds_train = tok_ds_train.rename_column(\"label\", \"labels\")\n",
    "tok_ds_test = tok_ds_test.rename_column(\"label\", \"labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create the data loaders for the train and test sets. As a collate function (i.e. the function used by a data loader to build batches from instances sampled from a dataset), we use the `DataCollatorWithPadding` class from the `transformers`library. It automatically performs dynamic (batch-wise) padding. It can do this because it recognizes the relevant \"keys\" that encode the input texts, i.e. the dataset column names `input_ids` and `attention_mask`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=model.lmtokenizer, padding=True, return_tensors='pt')\n",
    "\n",
    "# Divides the data into batches and places them in an iterator\n",
    "train_dataloader = DataLoader(tok_ds_train, shuffle=True, batch_size=8, collate_fn=data_collator)\n",
    "eval_dataloader = DataLoader(tok_ds_test, batch_size=8, collate_fn=data_collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.optim.lr_scheduler.LambdaLR at 0x1ba1899ee40>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# device = 'cpu'\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 717/717 [10:36<00:00,  1.27it/s]"
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        predictions = model(batch)\n",
    "        loss = model.loss_fn(predictions, batch['labels'].float())\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, texts):\n",
    "  model.eval()\n",
    "  encoded_texts = model.lmtokenizer(texts, truncation=True, padding=True, return_attention_mask=True, return_tensors='pt')\n",
    "  with torch.no_grad():\n",
    "    output = model(encoded_texts.to(device)).tolist()\n",
    "    pred_labels = [\"positive\" if p>0.5 else \"negative\" for p in output]\n",
    "    return list(zip(texts, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('That was a horrible movie!', 'negative'),\n",
       " ('I really liked it! Nice acting.', 'negative')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\n",
    "    \"That was a horrible movie!\",\n",
    "    \"I really liked it! Nice acting.\",\n",
    "]\n",
    "\n",
    "predict(model, texts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
