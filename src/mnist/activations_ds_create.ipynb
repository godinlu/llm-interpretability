{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création du dataset\n",
    "\n",
    "Ce notebook sert à créer un dataset d'activations de neuronnes. Pour créer ce dataset la première étape est d'entraîner un MLP sur le dataset MNIST, une fois le modèle entraîné on va faire de la prédiction tout en récupérant l'activation des neuronnes. Et enfin on sauvegarde ces activations dans un fichier CSV contenant les variables suivantes :\n",
    "- `nb_true` : Vrai nombre\n",
    "- `nb_pred` : Nombre qui a été prédit par le modèle\n",
    "- `activation1` : activation du neuronne 1 \n",
    "- `activation2` : activation du neuronne 2\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cette variable indique les nombres de neuronnes et le nombre de couche caché \n",
    "# ici [128, 64] indique 2 couches caché avec une couche de 128 neuronnes et une autres de 64 \n",
    "hidden_sizes = [128, 64] \n",
    "\n",
    "# Nombre d'épochs de l'entraînement\n",
    "epochs = 5\n",
    "\n",
    "# learning_rate pour l'apprentissage\n",
    "learning_rate = 0.001\n",
    "\n",
    "# nom du fichier de sortie, IMPORTANT : doit être un fichier csv ex : \"activations.csv\"\n",
    "output_file = \"../../data/activations/activations.csv\"\n",
    "\n",
    "# taille des batchs\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparation des données MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparer les données MNIST\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normaliser entre -1 et 1\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='../../data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='../../data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création du modèle MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size=784, hidden_sizes=[128, 64], output_size=10):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        in_features = input_size\n",
    "        for h in hidden_sizes:\n",
    "            self.hidden_layers.append(nn.Linear(in_features, h))\n",
    "            in_features = h\n",
    "        self.output_layer = nn.Linear(in_features, output_size)\n",
    "        self.activations = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.activations = []  # Réinitialiser les activations à chaque passage\n",
    "        x = x.view(x.size(0), -1)  # Aplatir l'image en vecteur\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x)\n",
    "            x = torch.relu(x)\n",
    "            self.activations.append(x.clone())  # Sauvegarder les activations après ReLU\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(hidden_sizes=hidden_sizes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraînement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Époch 1/5, Perte: 0.39569791773361945\n",
      "Époch 2/5, Perte: 0.1842050273964273\n",
      "Époch 3/5, Perte: 0.13582855409809522\n",
      "Époch 4/5, Perte: 0.10901505915797564\n",
      "Époch 5/5, Perte: 0.09439217581995514\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Époch {epoch+1}/{epochs}, Perte: {running_loss/len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation et prédiction\n",
    "\n",
    "On évalue le modèle sur le jeux de données de test et on récupère les activations de neuronnes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "all_activations = []\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        all_predictions.extend(predictions.numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "        \n",
    "        # Collecter les activations des couches cachées\n",
    "        batch_activations = [activation.numpy() for activation in model.activations]\n",
    "        # Transposer pour obtenir (échantillons, activations par couche)\n",
    "        batch_activations = np.concatenate(batch_activations, axis=1)  # Concatène les activations des couches\n",
    "        all_activations.extend(batch_activations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sauvegarde des activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les activations ont été sauvegardées dans ../../data/activations/activations.csv.\n"
     ]
    }
   ],
   "source": [
    "# Préparer les données pour le CSV\n",
    "csv_data = []\n",
    "for true_label, pred_label, activation in zip(all_labels, all_predictions, all_activations):\n",
    "    # Ajouter les données sous forme [nb_true, nb_pred, activation1, activation2, ...]\n",
    "    csv_data.append([true_label, pred_label] + activation.tolist())\n",
    "\n",
    "# Sauvegarder les données dans un fichier CSV\n",
    "with open(output_file, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Écrire l'en-tête du fichier\n",
    "    header = [\"nb_true\", \"nb_pred\"] + [f\"activation{i+1}\" for i in range(len(csv_data[0]) - 2)]\n",
    "    writer.writerow(header)\n",
    "    # Écrire les données\n",
    "    writer.writerows(csv_data)\n",
    "\n",
    "print(f\"Les activations ont été sauvegardées dans {output_file}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
